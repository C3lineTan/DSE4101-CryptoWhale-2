{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea7aa2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x145244fb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math, random, time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c2acd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"../../Data/eth_final_df.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"timestamp\" not in df.columns:\n",
    "    raise ValueError(\"Expected a datetime column named 'timestamp' in the CSV.\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "df = df.sort_values(\"timestamp\").dropna(subset=[\"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "TARGET_CANDIDATES = [\"vol_future\", \"target\", \"y\"]\n",
    "target_col = None\n",
    "for c in TARGET_CANDIDATES:\n",
    "    if c in df.columns:\n",
    "        target_col = c\n",
    "        break\n",
    "if target_col is None:\n",
    "    raise ValueError(f\"Could not find a target column among {TARGET_CANDIDATES}. \"\n",
    "                     \"Please rename your target to one of these or edit the list.\")\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in [\"timestamp\", target_col]]\n",
    "num_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "X_raw = df[[\"timestamp\"] + num_cols].copy()\n",
    "y_raw = df[[\"timestamp\", target_col]].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4894a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Random Synthesizer Attention\n",
    "# -----------------------------\n",
    "\n",
    "class MultiheadRandomSynthesizer(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, attn_dropout: float = 0.1, causal: bool = False):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.causal = causal\n",
    "\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def _shape(self, x: torch.Tensor, B: int, L: int) -> torch.Tensor:\n",
    "        return x.view(B, L, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        need_weights: bool = False,\n",
    "        attn_mask: Optional[torch.Tensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        B, L, D = x.size()\n",
    "        V = self._shape(self.v_proj(x), B, L) \n",
    "\n",
    "        rand_logits = torch.rand(B, self.num_heads, L, L, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = torch.triu(torch.ones(L, L, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "            rand_logits = rand_logits.masked_fill(causal_mask, float(\"-inf\"))\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            rand_logits = rand_logits.masked_fill(attn_mask.bool().unsqueeze(0).unsqueeze(0), float(\"-inf\"))\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            kpm = key_padding_mask.bool().unsqueeze(1).unsqueeze(1)  # [B,1,1,L]\n",
    "            rand_logits = rand_logits.masked_fill(kpm, float(\"-inf\"))\n",
    "\n",
    "        attn = F.softmax(rand_logits, dim=-1) \n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, V)  \n",
    "        out = out.transpose(1, 2).contiguous().view(B, L, D) \n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out, (attn if need_weights else None)\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# Encoder Layer using Random Synthesizer\n",
    "# -------------------------------------\n",
    "\n",
    "class RandomSynthesizerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        dim_feedforward: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "        attn_dropout: float = 0.1,\n",
    "        causal: bool = False,\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadRandomSynthesizer(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=nhead,\n",
    "            attn_dropout=attn_dropout,\n",
    "            causal=causal,\n",
    "        )\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,                     \n",
    "        src_key_padding_mask: Optional[torch.Tensor] = None,  \n",
    "        need_weights: bool = False,\n",
    "    ):\n",
    "        attn_out, attn_w = self.self_attn(src, key_padding_mask=src_key_padding_mask, need_weights=need_weights)\n",
    "        src = src + self.dropout1(attn_out)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        ffn = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(ffn)\n",
    "        src = self.norm2(src)\n",
    "\n",
    "        return src, attn_w\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Random Synthesizer Transformer\n",
    "# -----------------------------\n",
    "\n",
    "class RandomSynthesizerTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        d_model: int = 96,\n",
    "        nhead: int = 2,\n",
    "        num_layers: int = 1,\n",
    "        dim_feedforward: int = 384,\n",
    "        dropout: float = 0.1,\n",
    "        attn_dropout: float = 0.1,\n",
    "        causal: bool = False,\n",
    "        out_dim: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(input_dim, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            RandomSynthesizerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=dropout,\n",
    "                attn_dropout=attn_dropout,\n",
    "                causal=causal,\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_head = nn.Linear(d_model, out_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,                        \n",
    "        key_padding_mask: Optional[torch.Tensor] = None,  \n",
    "        return_attn: bool = False,\n",
    "    ):\n",
    "        attns = []\n",
    "        h = self.in_proj(x)                      \n",
    "        for layer in self.layers:\n",
    "            h, a = layer(h, src_key_padding_mask=key_padding_mask, need_weights=return_attn)\n",
    "            if return_attn:\n",
    "                attns.append(a)\n",
    "\n",
    "        last = h[:, -1, :]                       \n",
    "        y = self.out_head(self.dropout(last))    \n",
    "        return (y, attns) if return_attn else y\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Tiny glue to integrate quickly\n",
    "# -----------------------------\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    seq_len: int = 12\n",
    "    batch_size: int = 8\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    max_epochs: int = 60\n",
    "    patience: int = 6\n",
    "    d_model: int = 96\n",
    "    nhead: int = 2\n",
    "    num_layers: int = 1\n",
    "    dim_feedforward: int = 384\n",
    "    dropout: float = 0.3\n",
    "    attn_dropout: float = 0.1\n",
    "    causal: bool = False\n",
    "\n",
    "def build_model_random_synth(input_dim: int, cfg: TrainConfig, out_dim: int = 1) -> nn.Module:\n",
    "    return RandomSynthesizerTransformer(\n",
    "        input_dim=input_dim,\n",
    "        d_model=cfg.d_model,\n",
    "        nhead=cfg.nhead,\n",
    "        num_layers=cfg.num_layers,\n",
    "        dim_feedforward=cfg.dim_feedforward,\n",
    "        dropout=cfg.dropout,\n",
    "        attn_dropout=cfg.attn_dropout,\n",
    "        causal=cfg.causal,\n",
    "        out_dim=out_dim,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4b20a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Metrics ----------\n",
    "def rmse(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def rmpse(y_true, y_pred, eps=1e-12):\n",
    "    y_true = np.asarray(y_true, float); y_pred = np.asarray(y_pred, float)\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return float(np.sqrt(np.mean(((y_pred - y_true) / denom) ** 2)) * 100.0)\n",
    "\n",
    "def qlike_variance(v_true, v_pred, eps=1e-12):\n",
    "    v_true = np.maximum(np.asarray(v_true, float), eps)\n",
    "    v_pred = np.maximum(np.asarray(v_pred, float), eps)\n",
    "    m = np.isfinite(v_true) & np.isfinite(v_pred)\n",
    "    v_true = np.maximum(v_true[m], eps)\n",
    "    v_pred = np.maximum(v_pred[m], eps)\n",
    "    ratio = v_true / v_pred\n",
    "    return float(np.mean(ratio - np.log(ratio) - 1.0))\n",
    "\n",
    "# ---------- Dataset ----------\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X_seq: np.ndarray, y_seq: np.ndarray):\n",
    "        self.X = torch.tensor(X_seq, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y_seq, dtype=torch.float32).view(-1, 1)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "# ---------- Windowing ----------\n",
    "def make_sequences(X_df: pd.DataFrame, y_df: pd.DataFrame, seq_len: int\n",
    "                  ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    assert np.all(X_df[\"timestamp\"].values == y_df[\"timestamp\"].values)\n",
    "    X_values = X_df.drop(columns=[\"timestamp\"]).values\n",
    "    y_values = y_df.drop(columns=[\"timestamp\"]).values.squeeze(-1)\n",
    "    times = X_df[\"timestamp\"].values\n",
    "\n",
    "    X_seq, y_seq, t_seq = [], [], []\n",
    "    for i in range(seq_len, len(X_values)):\n",
    "        X_seq.append(X_values[i-seq_len:i, :])\n",
    "        y_seq.append(y_values[i])\n",
    "        t_seq.append(times[i])\n",
    "    return np.array(X_seq), np.array(y_seq), np.array(t_seq)\n",
    "\n",
    "# ---------- Time splits with purge ----------\n",
    "def build_rolling_folds(t_seq: np.ndarray,\n",
    "                        n_folds: int = 5,\n",
    "                        purge_hours: int = 24\n",
    "                       ) -> List[Dict[str, np.ndarray]]:\n",
    "\n",
    "    t = pd.to_datetime(t_seq)\n",
    "    N = len(t)\n",
    "    cuts = np.linspace(0, N, n_folds + 1, dtype=int)  \n",
    "    folds = []\n",
    "    for k in range(n_folds):\n",
    "        train_end_idx = cuts[k]  \n",
    "        valid_end_idx = cuts[k+1]\n",
    "        if train_end_idx == 0: \n",
    "            continue\n",
    "\n",
    "        purge_until_time = t[train_end_idx - 1] + pd.Timedelta(hours=purge_hours)\n",
    "        j = train_end_idx\n",
    "        while j < valid_end_idx and t[j] < purge_until_time:\n",
    "            j += 1\n",
    "\n",
    "        train_idx = np.arange(0, train_end_idx)\n",
    "        valid_idx = np.arange(j, valid_end_idx)\n",
    "\n",
    "        if len(valid_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        folds.append({\"train_idx\": train_idx, \"valid_idx\": valid_idx})\n",
    "    return folds[:n_folds]\n",
    "\n",
    "# ---------- Training helpers ----------\n",
    "def train_one_epoch(model, loader, optim, loss_fn):\n",
    "    model.train()\n",
    "    total = 0.0; n = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        optim.zero_grad(); loss.backward(); optim.step()\n",
    "        total += loss.item() * len(xb); n += len(xb)\n",
    "    return total / max(n,1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    total = 0.0; n = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        total += loss.item() * len(xb); n += len(xb)\n",
    "        y_true.append(yb.detach().cpu().numpy().ravel())\n",
    "        y_pred.append(pred.detach().cpu().numpy().ravel())\n",
    "    y_true = np.concatenate(y_true) if y_true else np.array([])\n",
    "    y_pred = np.concatenate(y_pred) if y_pred else np.array([])\n",
    "    return (total / max(n,1)), y_true, y_pred\n",
    "\n",
    "def early_stop_train(model, train_loader, val_loader, cfg, verbose=False):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    best_val = float(\"inf\"); best_state = None; patience_left = cfg.patience\n",
    "    for epoch in range(cfg.max_epochs):\n",
    "        _ = train_one_epoch(model, train_loader, optim, loss_fn)\n",
    "        val_loss, yv_true, yv_pred = eval_epoch(model, val_loader, loss_fn)\n",
    "        if val_loss < best_val - 1e-8:\n",
    "            best_val = val_loss; best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "            patience_left = cfg.patience\n",
    "        else:\n",
    "            patience_left -= 1\n",
    "            if patience_left <= 0:\n",
    "                break\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "# ---------- Config + Search Space ----------\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    seq_len: int = 12\n",
    "    batch_size: int = 8\n",
    "    lr: float = 3e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    max_epochs: int = 60\n",
    "    patience: int = 6\n",
    "    d_model: int = 96\n",
    "    nhead: int = 2\n",
    "    num_layers: int = 1\n",
    "    dim_feedforward: int = 384\n",
    "    dropout: float = 0.3\n",
    "    attn_dropout: float = 0.1\n",
    "    causal: bool = False\n",
    "\n",
    "SEARCH_SPACE = {\n",
    "    \"seq_len\":         [1, 2, 6, 12, 24, 36, 48, 72],\n",
    "    \"batch_size\":      [8, 32, 64, 128],\n",
    "    \"lr\":              [1e-3, 3e-4, 1e-4],\n",
    "    \"weight_decay\":    [1e-5, 1e-4, 3e-4],\n",
    "    \"d_model\":         [64, 96, 128],\n",
    "    \"nhead\":           [2, 4, 8],\n",
    "    \"num_layers\":      [1, 2, 3],\n",
    "    \"dim_feedforward\": [128, 256, 384],\n",
    "    \"dropout\":         [0.1, 0.2, 0.3],\n",
    "    \"max_epochs\":      [40, 60],\n",
    "    \"patience\":        [6, 8],\n",
    "    \"attn_dropout\":    [0.0, 0.1],\n",
    "    \"nhead\":           [1, 2, 4, 7],\n",
    "\n",
    "}\n",
    "\n",
    "def sample_cfg() -> TrainConfig:\n",
    "    def s(k): return random.choice(SEARCH_SPACE[k])\n",
    "    return TrainConfig(\n",
    "        seq_len=s(\"seq_len\"),\n",
    "        batch_size=s(\"batch_size\"),\n",
    "        lr=s(\"lr\"),\n",
    "        weight_decay=s(\"weight_decay\"),\n",
    "        d_model=s(\"d_model\"),\n",
    "        nhead=s(\"nhead\"),\n",
    "        num_layers=s(\"num_layers\"),\n",
    "        dim_feedforward=s(\"dim_feedforward\"),\n",
    "        dropout=s(\"dropout\"),\n",
    "        attn_dropout=s(\"attn_dropout\"),\n",
    "        patience=s(\"patience\"),\n",
    "        max_epochs=s(\"max_epochs\"),\n",
    "        causal=False,\n",
    "    )\n",
    "\n",
    "# ---------- Orchestration ----------\n",
    "def run_cv_random_synth(X_df: pd.DataFrame,\n",
    "                        y_df: pd.DataFrame,\n",
    "                        n_folds: int = 5,\n",
    "                        purge_hours: int = 24,\n",
    "                        n_trials: int = 25,\n",
    "                        metric_for_early_select: str = \"rmse\"  \n",
    "                       ):\n",
    "    cv_records = []\n",
    "    best_score = float(\"inf\")\n",
    "    best_cfg = None\n",
    "    best_trial_metrics = None\n",
    "\n",
    "    for trial in range(1, n_trials+1):\n",
    "        cfg = sample_cfg()\n",
    "\n",
    "        X_seq, y_seq, t_seq = make_sequences(X_df, y_df, seq_len=cfg.seq_len)\n",
    "        folds = build_rolling_folds(t_seq, n_folds=n_folds, purge_hours=purge_hours)\n",
    "        if len(folds) == 0:\n",
    "            raise ValueError(\"No valid folds produced. Check timestamps / seq_len.\")\n",
    "\n",
    "        fold_metrics = []\n",
    "        for k, f in enumerate(folds, 1):\n",
    "            tr_idx, va_idx = f[\"train_idx\"], f[\"valid_idx\"]\n",
    "            X_tr, y_tr = X_seq[tr_idx], y_seq[tr_idx]\n",
    "            X_va, y_va = X_seq[va_idx], y_seq[va_idx]\n",
    "\n",
    "            tr_loader = DataLoader(SeqDataset(X_tr, y_tr), batch_size=cfg.batch_size, shuffle=True, drop_last=False)\n",
    "            va_loader = DataLoader(SeqDataset(X_va, y_va), batch_size=cfg.batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "            model = build_model_random_synth(input_dim=X_seq.shape[-1], cfg=cfg, out_dim=1).to(DEVICE)\n",
    "            model = early_stop_train(model, tr_loader, va_loader, cfg)\n",
    "\n",
    "            _, yv_true, yv_pred = eval_epoch(model, va_loader, nn.MSELoss())\n",
    "            v_true = yv_true\n",
    "            v_pred = np.maximum(yv_pred, 1e-8)\n",
    "            v_true = np.square(v_true)\n",
    "            v_pred = np.square(v_pred)\n",
    "\n",
    "            fold_res = {\n",
    "                \"rmse\": rmse(v_true, v_pred),\n",
    "                \"rmpse\": rmpse(v_true, v_pred),\n",
    "                \"qlike\": qlike_variance(v_true, v_pred),\n",
    "            }\n",
    "            fold_metrics.append(fold_res)\n",
    "\n",
    "        avg = {m: float(np.mean([fm[m] for fm in fold_metrics])) for m in [\"rmse\",\"rmpse\",\"qlike\"]}\n",
    "        rec = {\"trial\": trial, **avg, **cfg.__dict__}\n",
    "        cv_records.append(rec)\n",
    "\n",
    "        key = metric_for_early_select\n",
    "        score = avg[key]\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_cfg = cfg\n",
    "            best_trial_metrics = {\"per_fold\": fold_metrics, \"avg\": avg}\n",
    "\n",
    "        print(f\"[Trial {trial:02d}] mean RMSE={avg['rmse']:.6f} QLIKE={avg['qlike']:.6f}  <- {'BEST' if score==best_score else ''}\")\n",
    "\n",
    "    cv_summary = pd.DataFrame(cv_records).sort_values(metric_for_early_select, ascending=True).reset_index(drop=True)\n",
    "    return best_cfg, cv_summary, best_trial_metrics\n",
    "\n",
    "# ---------- Final Train on full train span (optional test split by time) ----------\n",
    "def train_on_timespan(X_df, y_df, cfg: TrainConfig, train_until_time: Optional[pd.Timestamp]=None):\n",
    "    X_seq, y_seq, t_seq = make_sequences(X_df, y_df, seq_len=cfg.seq_len)\n",
    "    t_seq = pd.to_datetime(t_seq)\n",
    "\n",
    "    if train_until_time is None:\n",
    "        split_idx = int(len(X_seq) * 0.85)\n",
    "        tr_idx = np.arange(0, split_idx)\n",
    "        te_idx = np.arange(split_idx, len(X_seq))\n",
    "    else:\n",
    "        tr_idx = np.where(t_seq <= pd.Timestamp(train_until_time))[0]\n",
    "        te_idx = np.where(t_seq >  pd.Timestamp(train_until_time))[0]\n",
    "\n",
    "        if len(tr_idx) > 0 and len(te_idx) > 0:\n",
    "            purge_until = t_seq[tr_idx[-1]] + pd.Timedelta(hours=24)\n",
    "            te_idx = te_idx[t_seq[te_idx] >= purge_until]\n",
    "\n",
    "    X_tr, y_tr = X_seq[tr_idx], y_seq[tr_idx]\n",
    "    X_te, y_te = X_seq[te_idx], y_seq[te_idx]\n",
    "\n",
    "    tr_loader = DataLoader(SeqDataset(X_tr, y_tr), batch_size=cfg.batch_size, shuffle=True)\n",
    "    te_loader = DataLoader(SeqDataset(X_te, y_te), batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "    model = build_model_random_synth(input_dim=X_seq.shape[-1], cfg=cfg, out_dim=1).to(DEVICE)\n",
    "    model = early_stop_train(model, tr_loader, te_loader, cfg)\n",
    "\n",
    "    _, y_true, y_pred = eval_epoch(model, te_loader, nn.MSELoss())\n",
    "    v_true = y_true; v_pred = np.maximum(y_pred, 1e-8)\n",
    "    v_true = np.square(v_true)\n",
    "    v_pred = np.square(v_pred)\n",
    "    metrics = {\n",
    "        \"RMSE\": rmse(v_true, v_pred),\n",
    "        \"RMPSE\": rmpse(v_true, v_pred),\n",
    "        \"QLIKE\": qlike_variance(v_true, v_pred),\n",
    "        \"n_test\": int(len(v_true)),\n",
    "    }\n",
    "    return model, (v_true, v_pred, t_seq[te_idx]), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0421b1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 01] mean RMSE=0.000214 QLIKE=22964.311073  <- BEST\n",
      "[Trial 02] mean RMSE=0.000222 QLIKE=293363.290762  <- \n",
      "[Trial 03] mean RMSE=0.000258 QLIKE=29470609.192472  <- \n",
      "[Trial 04] mean RMSE=0.000217 QLIKE=420769.760768  <- \n",
      "[Trial 05] mean RMSE=0.000275 QLIKE=22008162.514865  <- \n",
      "[Trial 06] mean RMSE=0.000329 QLIKE=18258235.606144  <- \n",
      "[Trial 07] mean RMSE=0.000256 QLIKE=16912795.996858  <- \n",
      "[Trial 08] mean RMSE=0.000209 QLIKE=1255651.135352  <- BEST\n",
      "[Trial 09] mean RMSE=0.000219 QLIKE=59029.292100  <- \n",
      "[Trial 10] mean RMSE=0.000255 QLIKE=3800477.780690  <- \n",
      "[Trial 11] mean RMSE=0.000274 QLIKE=26813670.223014  <- \n",
      "[Trial 12] mean RMSE=0.000258 QLIKE=11719735.758099  <- \n",
      "[Trial 13] mean RMSE=0.000620 QLIKE=14661189.277899  <- \n",
      "[Trial 14] mean RMSE=0.000226 QLIKE=24386016.050515  <- \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "embed_dim must be divisible by num_heads",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_raw[X_raw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m cutoff_time]\n\u001b[1;32m      8\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_raw[y_raw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m cutoff_time]\n\u001b[0;32m---> 10\u001b[0m best_cfg, cv_table, best_cv \u001b[38;5;241m=\u001b[39m run_cv_random_synth(\n\u001b[1;32m     11\u001b[0m     X_df\u001b[38;5;241m=\u001b[39mX_train, y_df\u001b[38;5;241m=\u001b[39my_train,\n\u001b[1;32m     12\u001b[0m     n_folds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, purge_hours\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m,\n\u001b[1;32m     13\u001b[0m     n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,                  \n\u001b[1;32m     14\u001b[0m     metric_for_early_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest config:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_cfg)\n\u001b[1;32m     18\u001b[0m display(cv_table\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n",
      "Cell \u001b[0;32mIn[4], line 201\u001b[0m, in \u001b[0;36mrun_cv_random_synth\u001b[0;34m(X_df, y_df, n_folds, purge_hours, n_trials, metric_for_early_select)\u001b[0m\n\u001b[1;32m    198\u001b[0m tr_loader \u001b[38;5;241m=\u001b[39m DataLoader(SeqDataset(X_tr, y_tr), batch_size\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    199\u001b[0m va_loader \u001b[38;5;241m=\u001b[39m DataLoader(SeqDataset(X_va, y_va), batch_size\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 201\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model_random_synth(input_dim\u001b[38;5;241m=\u001b[39mX_seq\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], cfg\u001b[38;5;241m=\u001b[39mcfg, out_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    202\u001b[0m model \u001b[38;5;241m=\u001b[39m early_stop_train(model, tr_loader, va_loader, cfg)\n\u001b[1;32m    204\u001b[0m _, yv_true, yv_pred \u001b[38;5;241m=\u001b[39m eval_epoch(model, va_loader, nn\u001b[38;5;241m.\u001b[39mMSELoss())\n",
      "Cell \u001b[0;32mIn[3], line 175\u001b[0m, in \u001b[0;36mbuild_model_random_synth\u001b[0;34m(input_dim, cfg, out_dim)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_model_random_synth\u001b[39m(input_dim: \u001b[38;5;28mint\u001b[39m, cfg: TrainConfig, out_dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RandomSynthesizerTransformer(\n\u001b[1;32m    176\u001b[0m         input_dim\u001b[38;5;241m=\u001b[39minput_dim,\n\u001b[1;32m    177\u001b[0m         d_model\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39md_model,\n\u001b[1;32m    178\u001b[0m         nhead\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mnhead,\n\u001b[1;32m    179\u001b[0m         num_layers\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    180\u001b[0m         dim_feedforward\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdim_feedforward,\n\u001b[1;32m    181\u001b[0m         dropout\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[1;32m    182\u001b[0m         attn_dropout\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattn_dropout,\n\u001b[1;32m    183\u001b[0m         causal\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mcausal,\n\u001b[1;32m    184\u001b[0m         out_dim\u001b[38;5;241m=\u001b[39mout_dim,\n\u001b[1;32m    185\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[3], line 123\u001b[0m, in \u001b[0;36mRandomSynthesizerTransformer.__init__\u001b[0;34m(self, input_dim, d_model, nhead, num_layers, dim_feedforward, dropout, attn_dropout, causal, out_dim)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(input_dim, d_model)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m--> 123\u001b[0m     RandomSynthesizerEncoderLayer(\n\u001b[1;32m    124\u001b[0m         d_model\u001b[38;5;241m=\u001b[39md_model,\n\u001b[1;32m    125\u001b[0m         nhead\u001b[38;5;241m=\u001b[39mnhead,\n\u001b[1;32m    126\u001b[0m         dim_feedforward\u001b[38;5;241m=\u001b[39mdim_feedforward,\n\u001b[1;32m    127\u001b[0m         dropout\u001b[38;5;241m=\u001b[39mdropout,\n\u001b[1;32m    128\u001b[0m         attn_dropout\u001b[38;5;241m=\u001b[39mattn_dropout,\n\u001b[1;32m    129\u001b[0m         causal\u001b[38;5;241m=\u001b[39mcausal,\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)\n\u001b[1;32m    132\u001b[0m ])\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(dropout)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(d_model, out_dim)\n",
      "Cell \u001b[0;32mIn[3], line 70\u001b[0m, in \u001b[0;36mRandomSynthesizerEncoderLayer.__init__\u001b[0;34m(self, d_model, nhead, dim_feedforward, dropout, attn_dropout, causal, layer_norm_eps)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     61\u001b[0m     d_model: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m     layer_norm_eps: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-5\u001b[39m,\n\u001b[1;32m     68\u001b[0m ):\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m MultiheadRandomSynthesizer(\n\u001b[1;32m     71\u001b[0m         embed_dim\u001b[38;5;241m=\u001b[39md_model,\n\u001b[1;32m     72\u001b[0m         num_heads\u001b[38;5;241m=\u001b[39mnhead,\n\u001b[1;32m     73\u001b[0m         attn_dropout\u001b[38;5;241m=\u001b[39mattn_dropout,\n\u001b[1;32m     74\u001b[0m         causal\u001b[38;5;241m=\u001b[39mcausal,\n\u001b[1;32m     75\u001b[0m     )\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(d_model, dim_feedforward)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(dropout)\n",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36mMultiheadRandomSynthesizer.__init__\u001b[0;34m(self, embed_dim, num_heads, attn_dropout, causal)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim: \u001b[38;5;28mint\u001b[39m, num_heads: \u001b[38;5;28mint\u001b[39m, attn_dropout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m embed_dim \u001b[38;5;241m%\u001b[39m num_heads \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membed_dim must be divisible by num_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim \u001b[38;5;241m=\u001b[39m embed_dim\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m=\u001b[39m num_heads\n",
      "\u001b[0;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning with 5-fold purged rolling CV (24h)\n",
    "cutoff_time = pd.Timestamp(\"2025-08-23 16:00:00+00:00\")\n",
    "X_raw[\"timestamp\"] = pd.to_datetime(X_raw[\"timestamp\"], utc=True)\n",
    "y_raw[\"timestamp\"] = pd.to_datetime(y_raw[\"timestamp\"], utc=True)\n",
    "X_raw = X_raw.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "y_raw = y_raw.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "X_train = X_raw[X_raw[\"timestamp\"] < cutoff_time]\n",
    "y_train = y_raw[y_raw[\"timestamp\"] < cutoff_time]\n",
    "\n",
    "best_cfg, cv_table, best_cv = run_cv_random_synth(\n",
    "    X_df=X_train, y_df=y_train,\n",
    "    n_folds=5, purge_hours=24,\n",
    "    n_trials=20,                  \n",
    "    metric_for_early_select=\"rmse\" \n",
    ")\n",
    "\n",
    "print(\"Best config:\", best_cfg)\n",
    "display(cv_table.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf7c716",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_time = pd.Timestamp(\"2025-08-23 16:00:00+00:00\")\n",
    "final_model, (y_true, y_pred, t_test), test_metrics = train_on_timespan(\n",
    "    X_df=X_raw, y_df=y_raw, cfg=best_cfg, train_until_time=cutoff_time\n",
    ")\n",
    "print(\"Test metrics:\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dcadc9",
   "metadata": {},
   "source": [
    "Saving predicted result for backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da27f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\"timestamp\": pd.to_datetime(t_test), \"observed\": y_true, \"predicted\": y_pred})\n",
    "pred_df.to_csv(\"../../Results/predictions_random_synth_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
