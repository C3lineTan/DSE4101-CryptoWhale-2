{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1481,
   "id": "6380903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from statsmodels.tsa.api import VAR\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1482,
   "id": "b1db6305",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_dune_df = pd.read_csv('../Data/dune_btc_hour.csv')\n",
    "btc_whale_alerts_df = pd.read_csv('../Data/whale_alert_btc.csv')\n",
    "eth_dune_df = pd.read_csv('../Data/dune_eth_hour.csv')\n",
    "eth_whale_alerts_df = pd.read_csv('../Data/whale_alert_eth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1483,
   "id": "3d6afbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping realized_volatility column\n",
    "btc_dune_df = btc_dune_df.drop(columns = 'realized_volatility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1484,
   "id": "7cb7ffb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'RV_MA_1hr', 'RV_MA_3hr', 'RV_MA_12hr', 'vol_future',\n",
       "       'active_sending_addresses', 'active_receiving_addresses',\n",
       "       'exchange_withdrawing_count', 'transaction_count', 'fail_rate_percent',\n",
       "       'open', 'low', 'high', 'close', 'hourly_return',\n",
       "       'onchain_volume_usd_log', 'avg_gas_fee_usd_log',\n",
       "       'avg_priority_fee_usd_log', 'staking_inflow_log',\n",
       "       'exchange_depositing_count_log', 'exchange_netflow_usd_log'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_dune_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1485,
   "id": "65a45127",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_dune_df = btc_dune_df.rename(columns={'hour_utc': 'timestamp'})\n",
    "eth_dune_df = eth_dune_df.rename(columns={'datetime': 'timestamp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1486,
   "id": "8b67e073",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_dune_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf55eba",
   "metadata": {},
   "source": [
    "### VAR-FEVD\n",
    "\n",
    "We aim to analyze the dynamic relationship between Bitcoin and Ethereum volatility using a bivariate Vector Autoregression (VAR) model, followed by Forecast Error Variance Decomposition (FEVD).\n",
    "\n",
    "FEVD quantifies the spillover effect — e.g., whether ETH volatility or drives BTC volatility shocks over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1487,
   "id": "01e19284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitcoin columns: 8232, ethereum columns: 8232\n"
     ]
    }
   ],
   "source": [
    "btc_vol_df = pd.read_csv('../Data/btc_vol_future.csv')\n",
    "eth_vol_df = pd.read_csv('../Data/eth_vol_future.csv')\n",
    "\n",
    "print(f'bitcoin columns: {len(btc_vol_df)}, ethereum columns: {len(eth_vol_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1488,
   "id": "93da6aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>vol_future</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-04 00:00:00.000 UTC</td>\n",
       "      <td>0.009547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-04 01:00:00.000 UTC</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-04 02:00:00.000 UTC</td>\n",
       "      <td>0.009945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-04 03:00:00.000 UTC</td>\n",
       "      <td>0.009561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-04 04:00:00.000 UTC</td>\n",
       "      <td>0.008098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      datetime  vol_future\n",
       "0  2024-11-04 00:00:00.000 UTC    0.009547\n",
       "1  2024-11-04 01:00:00.000 UTC    0.012712\n",
       "2  2024-11-04 02:00:00.000 UTC    0.009945\n",
       "3  2024-11-04 03:00:00.000 UTC    0.009561\n",
       "4  2024-11-04 04:00:00.000 UTC    0.008098"
      ]
     },
     "execution_count": 1488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btc_vol_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1489,
   "id": "07a8adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming vol_future columns\n",
    "btc_vol_df = btc_vol_df.rename(columns={'vol_future': 'btc_vol_future'})\n",
    "eth_vol_df = eth_vol_df.rename(columns={'vol_future': 'eth_vol_future'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1490,
   "id": "27a928e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After merging vol data: 8232 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>eth_vol_future</th>\n",
       "      <th>btc_vol_future</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-04 00:00:00.000 UTC</td>\n",
       "      <td>0.008161</td>\n",
       "      <td>0.009547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-04 01:00:00.000 UTC</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-04 02:00:00.000 UTC</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>0.009945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-04 03:00:00.000 UTC</td>\n",
       "      <td>0.002562</td>\n",
       "      <td>0.009561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-04 04:00:00.000 UTC</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.008098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     timestamp  eth_vol_future  btc_vol_future\n",
       "0  2024-11-04 00:00:00.000 UTC        0.008161        0.009547\n",
       "1  2024-11-04 01:00:00.000 UTC        0.005250        0.012712\n",
       "2  2024-11-04 02:00:00.000 UTC        0.003669        0.009945\n",
       "3  2024-11-04 03:00:00.000 UTC        0.002562        0.009561\n",
       "4  2024-11-04 04:00:00.000 UTC        0.002531        0.008098"
      ]
     },
     "execution_count": 1490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol_merged_df = pd.merge(eth_vol_df, btc_vol_df, on='datetime', how='inner')\n",
    "print(f'After merging vol data: {len(vol_merged_df)} columns')\n",
    "\n",
    "vol_merged_df = vol_merged_df.rename(columns={'datetime': 'timestamp'})\n",
    "\n",
    "vol_merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1491,
   "id": "53a172bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp         0\n",
       "eth_vol_future    0\n",
       "btc_vol_future    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol_merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1492,
   "id": "1cb4cbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in vol_merged_df.columns:\n",
    "    if col != 'timestamp':\n",
    "        lower_limit = vol_merged_df[col].quantile(0.05)\n",
    "        upper_limit = vol_merged_df[col].quantile(0.95)\n",
    "        vol_merged_df[col] = np.clip(vol_merged_df[col], lower_limit, upper_limit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc93201",
   "metadata": {},
   "source": [
    "### Code for VAR-FEVD \n",
    "We will do a rolling window to calculate the spill effects, each var model computed in each row will use up to the previous 24-hour data, the lags are determined BIC\n",
    "\n",
    "\n",
    "We will use bic: https://stats.stackexchange.com/questions/313586/var-lag-selection-tests-which-one-do-i-choose\n",
    "Response is verified using stock and watson paper: https://www.princeton.edu/~mwatson/papers/Stock_Watson_HOM_Vol2.pdf\n",
    "we also want a more conservative approach to avoid overfitting so bic will be a more appropriate choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1493,
   "id": "e01d85e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA QUALITY DIAGNOSTICS\n",
      "============================================================\n",
      "  total_rows: 8232\n",
      "  btc_null_count: 0\n",
      "  eth_null_count: 0\n",
      "  btc_zero_count: 0\n",
      "  eth_zero_count: 0\n",
      "  btc_mean: 0.006379\n",
      "  eth_mean: 0.005875\n",
      "  btc_std: 0.003893\n",
      "  eth_std: 0.002958\n",
      "  btc_min: 0.001607\n",
      "  eth_min: 0.002317\n",
      "  btc_max: 0.014931\n",
      "  eth_max: 0.013275\n",
      "  correlation: 0.431853\n",
      "  constant_windows: 22\n",
      "  constant_window_pct: 0.268031\n",
      "\n",
      "============================================================\n",
      "COMPUTING SPILLOVER FEATURES\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing VAR-FEVD spillovers:  96%|█████████▌| 7844/8208 [00:10<00:00, 798.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag selection error at 7694: x contains one or more constant columns. Column(s)\n",
      "Lag selection error at 7695: x contains one or more constant columns. Column(s)\n",
      "Lag selection error at 7696: x contains one or more constant columns. Column(s)\n",
      "Lag selection error at 7697: x contains one or more constant columns. Column(s)\n",
      "Lag selection error at 7698: x contains one or more constant columns. Column(s)\n",
      "Lag selection error at 7721: x contains one or more constant columns. Column(s)\n",
      "ValueError at 7721: x contains one or more constant columns. Column(s) 0 are constant. Adding a constant with trend='c' \n",
      "Lag selection error at 7722: x contains one or more constant columns. Column(s)\n",
      "Lag selection error at 7723: x contains one or more constant columns. Column(s)\n",
      "Lag selection error at 7724: x contains one or more constant columns. Column(s)\n",
      "Lag selection error at 7725: x contains one or more constant columns. Column(s)\n",
      "Lag selection error at 7726: x contains one or more constant columns. Column(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing VAR-FEVD spillovers: 100%|██████████| 8208/8208 [00:10<00:00, 765.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VAR MODEL DIAGNOSTICS\n",
      "============================================================\n",
      "Success count: 7093\n",
      "Total attempts: 8208\n",
      "Success rate: 86.42%\n",
      "\n",
      "Error breakdown:\n",
      "  unstable_model: 1092 (13.3%)\n",
      "  zero_variance: 22 (0.3%)\n",
      "  lag_selection_error: 11 (0.1%)\n",
      "  value_error: 1 (0.0%)\n",
      "\n",
      "============================================================\n",
      "SPILLOVER FEATURE DIAGNOSTICS\n",
      "============================================================\n",
      "  total_observations: 8232\n",
      "  valid_spillover_obs: 7093\n",
      "  coverage_pct: 86.1638\n",
      "  btc_to_eth_mean: 0.3024\n",
      "  eth_to_btc_mean: 0.0000\n",
      "  btc_to_eth_std: 0.2444\n",
      "  eth_to_btc_std: 0.0000\n",
      "  correlation: nan\n",
      "\n",
      "Final dataset shape: (8232, 4)\n",
      "Spillover columns added: ['btc_to_eth_spill', 'eth_to_btc_spill']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.api import VAR\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "def compute_var_fevd_features(df, window=24, horizon=1, maxlags=12, \n",
    "                               min_samples=None, fixed_lag=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Compute volatility spillover features between BTC and ETH using VAR-FEVD.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain 'btc_vol_future' and 'eth_vol_future' columns, indexed by timestamp.\n",
    "    window : int, default=24\n",
    "        Rolling window size (number of observations).\n",
    "    horizon : int, default=1\n",
    "        Forecast horizon for FEVD (steps ahead for decomposition).\n",
    "    maxlags : int, default=12\n",
    "        Maximum lags for VAR model selection (if fixed_lag not specified).\n",
    "    min_samples : int, optional\n",
    "        Minimum samples required to fit model. If None, uses max(10, window//2).\n",
    "    fixed_lag : int, optional\n",
    "        If specified, uses fixed lag order instead of BIC selection (faster).\n",
    "    verbose : bool, default=False\n",
    "        If True, prints detailed error diagnostics.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns: ['btc_to_eth_spill', 'eth_to_btc_spill']\n",
    "        Same index as input df.\n",
    "    dict\n",
    "        Diagnostic information about failures\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare data\n",
    "    df_work = df[['btc_vol_future', 'eth_vol_future']].copy()\n",
    "    df_work = df_work.sort_index()\n",
    "    \n",
    "    # Check for sufficient data\n",
    "    non_nan_count = len(df_work.dropna())\n",
    "    if non_nan_count < window:\n",
    "        raise ValueError(f\"Insufficient data: need at least {window} non-NaN rows, found {non_nan_count}\")\n",
    "    \n",
    "    # Set minimum samples\n",
    "    if min_samples is None:\n",
    "        min_samples = max(10, window // 2)\n",
    "    \n",
    "    # Initialize output arrays\n",
    "    n = len(df_work)\n",
    "    btc_to_eth_list = np.full(n, np.nan)\n",
    "    eth_to_btc_list = np.full(n, np.nan)\n",
    "    \n",
    "    # Diagnostics\n",
    "    error_counter = Counter()\n",
    "    success_count = 0\n",
    "    \n",
    "    # Rolling window computation\n",
    "    for i in tqdm(range(window, n), desc=\"Computing VAR-FEVD spillovers\", \n",
    "                  disable=not verbose):\n",
    "        \n",
    "        # Extract window\n",
    "        sub_df = df_work.iloc[i - window:i].dropna()\n",
    "        \n",
    "        # Check sufficient data\n",
    "        if len(sub_df) < min_samples:\n",
    "            error_counter['insufficient_samples'] += 1\n",
    "            continue\n",
    "        \n",
    "        if sub_df.shape[1] < 2:\n",
    "            error_counter['missing_columns'] += 1\n",
    "            continue\n",
    "        \n",
    "        # Check for zero variance\n",
    "        if (sub_df.std() == 0).any():\n",
    "            error_counter['zero_variance'] += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Fit VAR model\n",
    "            model = VAR(sub_df)\n",
    "            \n",
    "            # Determine lag order\n",
    "            if fixed_lag is not None:\n",
    "                p = fixed_lag\n",
    "            else:\n",
    "                maxlags_eff = min(maxlags, len(sub_df) // 3)\n",
    "                if maxlags_eff < 1:\n",
    "                    error_counter['window_too_small'] += 1\n",
    "                    continue\n",
    "                \n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    try:\n",
    "                        sel = model.select_order(maxlags=maxlags_eff)\n",
    "                        p = sel.selected_orders.get('bic')\n",
    "                        p = max(1, min(p, maxlags_eff))\n",
    "                    except Exception as e:\n",
    "                        error_counter[f'lag_selection_error'] += 1\n",
    "                        if verbose:\n",
    "                            print(f\"Lag selection error at {i}: {str(e)[:50]}\")\n",
    "                        p = 1\n",
    "            \n",
    "            # Check if we have enough data for chosen lag\n",
    "            if len(sub_df) < p * 2 + 5:\n",
    "                error_counter['insufficient_for_lag'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Fit model with chosen lag\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                results = model.fit(p)\n",
    "            \n",
    "            # Check stability\n",
    "            if not results.is_stable():\n",
    "                error_counter['unstable_model'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Compute FEVD\n",
    "            fevd = results.fevd(horizon)\n",
    "            \n",
    "            # FEVD decomp shape is (steps, variables, shocks) \n",
    "            # But actual shape can be (2, 1, 2) or (1, 2, 2) depending on statsmodels version\n",
    "            # We need a (variables, shocks) matrix = (2, 2)\n",
    "            \n",
    "            decomp_shape = fevd.decomp.shape\n",
    "            \n",
    "            # Handle different possible shapes\n",
    "            if decomp_shape == (2, 2):\n",
    "                # Already correct 2D shape\n",
    "                table = fevd.decomp\n",
    "            elif len(decomp_shape) == 3:\n",
    "                # 3D array - need to extract the right slice\n",
    "                if decomp_shape[0] == 2 and decomp_shape[1] == 1 and decomp_shape[2] == 2:\n",
    "                    # Shape is (variables=2, steps=1, shocks=2) - squeeze middle dim\n",
    "                    table = fevd.decomp[:, 0, :]  # Result: (2, 2)\n",
    "                elif decomp_shape[0] == 1 and decomp_shape[1] == 2 and decomp_shape[2] == 2:\n",
    "                    # Shape is (steps=1, variables=2, shocks=2)\n",
    "                    table = fevd.decomp[0, :, :]  # Result: (2, 2)\n",
    "                elif decomp_shape[0] >= horizon and decomp_shape[1] == 2 and decomp_shape[2] == 2:\n",
    "                    # Standard shape (steps, variables, shocks)\n",
    "                    table = fevd.decomp[horizon - 1, :, :]\n",
    "                else:\n",
    "                    error_counter[f'unexpected_shape_{decomp_shape}'] += 1\n",
    "                    if verbose and error_counter[f'unexpected_shape_{decomp_shape}'] <= 3:\n",
    "                        print(f\"Cannot handle decomp shape at {i}: {decomp_shape}\")\n",
    "                    continue\n",
    "            else:\n",
    "                error_counter[f'invalid_ndim_{len(decomp_shape)}'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Validate final table shape\n",
    "            if table.shape != (2, 2):\n",
    "                error_counter[f'wrong_table_shape_{table.shape}'] += 1\n",
    "                if verbose and error_counter[f'wrong_table_shape_{table.shape}'] <= 3:\n",
    "                    print(f\"Table shape at {i}: {table.shape} from decomp {decomp_shape}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract spillover effects\n",
    "            # table[i,j] = contribution of shock to variable j on forecast error of variable i\n",
    "            btc_to_eth = table[1, 0]\n",
    "            eth_to_btc = table[0, 1]\n",
    "            \n",
    "            # Sanity check: values should be between 0 and 1\n",
    "            if not (0 <= btc_to_eth <= 1 and 0 <= eth_to_btc <= 1):\n",
    "                error_counter['invalid_fevd_values'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Assign to current timestamp\n",
    "            btc_to_eth_list[i] = btc_to_eth\n",
    "            eth_to_btc_list[i] = eth_to_btc\n",
    "            success_count += 1\n",
    "            \n",
    "        except np.linalg.LinAlgError:\n",
    "            error_counter['linalg_error'] += 1\n",
    "        except ValueError as e:\n",
    "            error_counter['value_error'] += 1\n",
    "            if verbose and error_counter['value_error'] <= 3:\n",
    "                print(f\"ValueError at {i}: {str(e)[:100]}\")\n",
    "        except Exception as e:\n",
    "            error_type = type(e).__name__\n",
    "            error_counter[f'other_{error_type}'] += 1\n",
    "            if verbose and error_counter[f'other_{error_type}'] <= 3:\n",
    "                print(f\"{error_type} at {i}: {str(e)[:100]}\")\n",
    "    \n",
    "    # Create output dataframe\n",
    "    result = pd.DataFrame({\n",
    "        'btc_to_eth_spill': btc_to_eth_list,\n",
    "        'eth_to_btc_spill': eth_to_btc_list\n",
    "    }, index=df_work.index)\n",
    "    \n",
    "    # Diagnostics\n",
    "    diagnostics = {\n",
    "        'success_count': success_count,\n",
    "        'total_attempts': n - window,\n",
    "        'success_rate': success_count / (n - window) if n > window else 0,\n",
    "        'errors': dict(error_counter)\n",
    "    }\n",
    "    \n",
    "    return result, diagnostics\n",
    "\n",
    "\n",
    "def diagnose_data(df):\n",
    "    \"\"\"\n",
    "    Diagnose data quality issues that might prevent VAR-FEVD from working.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with volatility columns\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Diagnostic report\n",
    "    \"\"\"\n",
    "    df_work = df[['btc_vol_future', 'eth_vol_future']].copy()\n",
    "    \n",
    "    report = {\n",
    "        'total_rows': len(df_work),\n",
    "        'btc_null_count': df_work['btc_vol_future'].isna().sum(),\n",
    "        'eth_null_count': df_work['eth_vol_future'].isna().sum(),\n",
    "        'btc_zero_count': (df_work['btc_vol_future'] == 0).sum(),\n",
    "        'eth_zero_count': (df_work['eth_vol_future'] == 0).sum(),\n",
    "        'btc_mean': df_work['btc_vol_future'].mean(),\n",
    "        'eth_mean': df_work['eth_vol_future'].mean(),\n",
    "        'btc_std': df_work['btc_vol_future'].std(),\n",
    "        'eth_std': df_work['eth_vol_future'].std(),\n",
    "        'btc_min': df_work['btc_vol_future'].min(),\n",
    "        'eth_min': df_work['eth_vol_future'].min(),\n",
    "        'btc_max': df_work['btc_vol_future'].max(),\n",
    "        'eth_max': df_work['eth_vol_future'].max(),\n",
    "        'correlation': df_work['btc_vol_future'].corr(df_work['eth_vol_future']),\n",
    "    }\n",
    "    \n",
    "    # Check for constant values in rolling windows\n",
    "    window = 24\n",
    "    constant_windows = 0\n",
    "    for i in range(window, len(df_work)):\n",
    "        sub = df_work.iloc[i-window:i].dropna()\n",
    "        if len(sub) > 0 and ((sub.std() == 0).any()):\n",
    "            constant_windows += 1\n",
    "    \n",
    "    report['constant_windows'] = constant_windows\n",
    "    report['constant_window_pct'] = constant_windows / max(1, len(df_work) - window) * 100\n",
    "    \n",
    "    return report\n",
    "\n",
    "\n",
    "def add_spillover_diagnostics(df, spillover_df):\n",
    "    \"\"\"Add diagnostic statistics about spillover features.\"\"\"\n",
    "    stats = {\n",
    "        'total_observations': len(df),\n",
    "        'valid_spillover_obs': spillover_df['btc_to_eth_spill'].notna().sum(),\n",
    "        'coverage_pct': spillover_df['btc_to_eth_spill'].notna().sum() / len(df) * 100,\n",
    "        'btc_to_eth_mean': spillover_df['btc_to_eth_spill'].mean(),\n",
    "        'eth_to_btc_mean': spillover_df['eth_to_btc_spill'].mean(),\n",
    "        'btc_to_eth_std': spillover_df['btc_to_eth_spill'].std(),\n",
    "        'eth_to_btc_std': spillover_df['eth_to_btc_spill'].std(),\n",
    "        'correlation': spillover_df[['btc_to_eth_spill', 'eth_to_btc_spill']].corr().iloc[0, 1]\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "\n",
    "def test_fevd_structure(df, window=24, horizon=1, fixed_lag=1):\n",
    "    \"\"\"\n",
    "    Quick test to understand FEVD structure with your data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    window : int\n",
    "        Window size to test\n",
    "    horizon : int\n",
    "        Horizon to test\n",
    "    fixed_lag : int\n",
    "        Lag order to use\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Information about FEVD structure\n",
    "    \"\"\"\n",
    "    df_work = df[['btc_vol_future', 'eth_vol_future']].copy()\n",
    "    df_work = df_work.sort_index()\n",
    "    \n",
    "    # Find first valid window\n",
    "    for i in range(window, len(df_work)):\n",
    "        sub_df = df_work.iloc[i - window:i].dropna()\n",
    "        \n",
    "        if len(sub_df) >= window and (sub_df.std() != 0).all():\n",
    "            try:\n",
    "                model = VAR(sub_df)\n",
    "                results = model.fit(fixed_lag)\n",
    "                \n",
    "                if results.is_stable():\n",
    "                    fevd = results.fevd(horizon)\n",
    "                    \n",
    "                    info = {\n",
    "                        'fevd_decomp_shape': fevd.decomp.shape,\n",
    "                        'fevd_decomp_ndim': fevd.decomp.ndim,\n",
    "                        'fevd_type': type(fevd).__name__,\n",
    "                        'window_used': i,\n",
    "                        'sample_decomp': fevd.decomp,\n",
    "                        'variable_names': list(sub_df.columns),\n",
    "                    }\n",
    "                    \n",
    "                    print(\"FEVD Structure Test Results:\")\n",
    "                    print(\"=\" * 60)\n",
    "                    for key, value in info.items():\n",
    "                        if key != 'sample_decomp':\n",
    "                            print(f\"{key}: {value}\")\n",
    "                    \n",
    "                    print(\"\\nFull decomp array:\")\n",
    "                    print(fevd.decomp)\n",
    "                    \n",
    "                    print(\"\\nInterpretation:\")\n",
    "                    if fevd.decomp.ndim == 3:\n",
    "                        print(f\"  Shape: (steps={fevd.decomp.shape[0]}, \"\n",
    "                              f\"variables={fevd.decomp.shape[1]}, \"\n",
    "                              f\"shocks={fevd.decomp.shape[2]})\")\n",
    "                        print(f\"  For horizon {horizon}, use: fevd.decomp[{horizon-1}, :, :]\")\n",
    "                    else:\n",
    "                        print(f\"  Shape: (variables={fevd.decomp.shape[0]}, \"\n",
    "                              f\"shocks={fevd.decomp.shape[1]})\")\n",
    "                        print(f\"  Already a 2D matrix, use directly\")\n",
    "                    \n",
    "                    return info\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error at window {i}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(\"Could not find valid window for testing\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Example usage with diagnostics:\n",
    "if __name__ == \"__main__\":\n",
    "    # Prepare data\n",
    "    df = vol_merged_df.sort_values('timestamp').set_index('timestamp')\n",
    "    \n",
    "    # Step 1: Diagnose data quality\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA QUALITY DIAGNOSTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    data_report = diagnose_data(df)\n",
    "    for key, value in data_report.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.6f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Step 2: Compute spillover features with verbose output\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COMPUTING SPILLOVER FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    spillover_features, var_diagnostics = compute_var_fevd_features(\n",
    "        df, \n",
    "        window=24, \n",
    "        horizon=1,\n",
    "        maxlags=6,  # Reduced from 12 - try smaller first\n",
    "        fixed_lag=None,  # Use BIC selection\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Step 3: Print error breakdown\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"VAR MODEL DIAGNOSTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Success count: {var_diagnostics['success_count']}\")\n",
    "    print(f\"Total attempts: {var_diagnostics['total_attempts']}\")\n",
    "    print(f\"Success rate: {var_diagnostics['success_rate']:.2%}\")\n",
    "    \n",
    "    if var_diagnostics['errors']:\n",
    "        print(\"\\nError breakdown:\")\n",
    "        for error, count in sorted(var_diagnostics['errors'].items(), \n",
    "                                   key=lambda x: x[1], reverse=True):\n",
    "            pct = count / var_diagnostics['total_attempts'] * 100\n",
    "            print(f\"  {error}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Step 4: Feature diagnostics\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SPILLOVER FEATURE DIAGNOSTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    feature_stats = add_spillover_diagnostics(df, spillover_features)\n",
    "    for key, value in feature_stats.items():\n",
    "        if isinstance(value, float) and not np.isnan(value):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Merge with original data\n",
    "    ml_ready_df = df.join(spillover_features)\n",
    "    \n",
    "    print(f\"\\nFinal dataset shape: {ml_ready_df.shape}\")\n",
    "    print(f\"Spillover columns added: {spillover_features.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15309e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 1494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_ready_df =ml_ready_df.reset_index()\n",
    "\n",
    "final_spillover_df = ml_ready_df[ml_ready_df['timestamp'] >= '2024-11-05']\n",
    "\n",
    "max(final_spillover_df['eth_to_btc_spill'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1501,
   "id": "9882f618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>eth_vol_future</th>\n",
       "      <th>btc_vol_future</th>\n",
       "      <th>btc_to_eth_spill</th>\n",
       "      <th>eth_to_btc_spill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2024-11-05 00:00:00.000 UTC</td>\n",
       "      <td>0.009336</td>\n",
       "      <td>0.008782</td>\n",
       "      <td>0.032570</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2024-11-05 01:00:00.000 UTC</td>\n",
       "      <td>0.003813</td>\n",
       "      <td>0.005309</td>\n",
       "      <td>0.061816</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2024-11-05 02:00:00.000 UTC</td>\n",
       "      <td>0.003502</td>\n",
       "      <td>0.006122</td>\n",
       "      <td>0.080956</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2024-11-05 03:00:00.000 UTC</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.006630</td>\n",
       "      <td>0.129080</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2024-11-05 04:00:00.000 UTC</td>\n",
       "      <td>0.003244</td>\n",
       "      <td>0.007759</td>\n",
       "      <td>0.159381</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8227</th>\n",
       "      <td>2025-10-12 19:00:00.000 UTC</td>\n",
       "      <td>0.007959</td>\n",
       "      <td>0.006841</td>\n",
       "      <td>0.498550</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8228</th>\n",
       "      <td>2025-10-12 20:00:00.000 UTC</td>\n",
       "      <td>0.013275</td>\n",
       "      <td>0.008244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8229</th>\n",
       "      <td>2025-10-12 21:00:00.000 UTC</td>\n",
       "      <td>0.007669</td>\n",
       "      <td>0.006577</td>\n",
       "      <td>0.336595</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8230</th>\n",
       "      <td>2025-10-12 22:00:00.000 UTC</td>\n",
       "      <td>0.009990</td>\n",
       "      <td>0.006362</td>\n",
       "      <td>0.304397</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8231</th>\n",
       "      <td>2025-10-12 23:00:00.000 UTC</td>\n",
       "      <td>0.010211</td>\n",
       "      <td>0.004646</td>\n",
       "      <td>0.323808</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8208 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        timestamp  eth_vol_future  btc_vol_future  \\\n",
       "24    2024-11-05 00:00:00.000 UTC        0.009336        0.008782   \n",
       "25    2024-11-05 01:00:00.000 UTC        0.003813        0.005309   \n",
       "26    2024-11-05 02:00:00.000 UTC        0.003502        0.006122   \n",
       "27    2024-11-05 03:00:00.000 UTC        0.003399        0.006630   \n",
       "28    2024-11-05 04:00:00.000 UTC        0.003244        0.007759   \n",
       "...                           ...             ...             ...   \n",
       "8227  2025-10-12 19:00:00.000 UTC        0.007959        0.006841   \n",
       "8228  2025-10-12 20:00:00.000 UTC        0.013275        0.008244   \n",
       "8229  2025-10-12 21:00:00.000 UTC        0.007669        0.006577   \n",
       "8230  2025-10-12 22:00:00.000 UTC        0.009990        0.006362   \n",
       "8231  2025-10-12 23:00:00.000 UTC        0.010211        0.004646   \n",
       "\n",
       "      btc_to_eth_spill  eth_to_btc_spill  \n",
       "24            0.032570               0.0  \n",
       "25            0.061816               0.0  \n",
       "26            0.080956               0.0  \n",
       "27            0.129080               0.0  \n",
       "28            0.159381               0.0  \n",
       "...                ...               ...  \n",
       "8227          0.498550               0.0  \n",
       "8228               NaN               NaN  \n",
       "8229          0.336595               0.0  \n",
       "8230          0.304397               0.0  \n",
       "8231          0.323808               0.0  \n",
       "\n",
       "[8208 rows x 5 columns]"
      ]
     },
     "execution_count": 1501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_spillover_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb366df",
   "metadata": {},
   "source": [
    "### Final merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1495,
   "id": "58ffedf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eth_whale_alerts_df['datetime1h'] = pd.to_datetime(eth_whale_alerts_df['datetime1h'], utc=True)\n",
    "eth_dune_df['timestamp'] = pd.to_datetime(eth_dune_df['timestamp'], utc=True)\n",
    "\n",
    "eth_whale_alerts_df = eth_whale_alerts_df.rename(columns={'datetime1h': 'timestamp'})\n",
    "\n",
    "eth_whale_alerts_df = eth_whale_alerts_df.sort_values('timestamp')\n",
    "eth_dune_df = eth_dune_df.sort_values('timestamp')\n",
    "\n",
    "eth_merged_df = pd.merge_asof(\n",
    "    eth_dune_df,\n",
    "    eth_whale_alerts_df,\n",
    "    on='timestamp',\n",
    "    direction='nearest',       # optional: use nearest timestamp match\n",
    "    tolerance=pd.Timedelta('1h')  # optional: only merge if within 1 hour\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1496,
   "id": "08ebf4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                           0\n",
       "RV_MA_1hr                           0\n",
       "RV_MA_3hr                           0\n",
       "RV_MA_12hr                          0\n",
       "vol_future                          0\n",
       "active_sending_addresses            0\n",
       "active_receiving_addresses          0\n",
       "exchange_withdrawing_count          0\n",
       "transaction_count                   0\n",
       "fail_rate_percent                   0\n",
       "open                                0\n",
       "low                                 0\n",
       "high                                0\n",
       "close                               0\n",
       "hourly_return                       0\n",
       "onchain_volume_usd_log              0\n",
       "avg_gas_fee_usd_log                 0\n",
       "avg_priority_fee_usd_log            0\n",
       "staking_inflow_log                  0\n",
       "exchange_depositing_count_log       0\n",
       "exchange_netflow_usd_log            0\n",
       "whale_net_usd                    6383\n",
       "whale_net_usd_24h                6383\n",
       "whale_burst_flag                 6383\n",
       "etow_usd_log                     6383\n",
       "etow_coins_log                   6383\n",
       "whale_txn_count_log              6383\n",
       "wtoe_usd_log                     6383\n",
       "wtoe_coins_log                   6383\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eth_merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1497,
   "id": "bc251019",
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_merged_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1498,
   "id": "7c495131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp                        0\n",
       "btc_exchange_netflow_usd         0\n",
       "active_sending_addresses         0\n",
       "active_receiving_addresses       0\n",
       "onchain_volume_usd               0\n",
       "open                             0\n",
       "low                              0\n",
       "high                             0\n",
       "close                            0\n",
       "mint_reward_usd                  0\n",
       "total_fee_usd                    0\n",
       "transaction_count                0\n",
       "exchange_to_wallet_usd           0\n",
       "wallet_to_exchange_usd           0\n",
       "RV_MA_1hr                        0\n",
       "RV_MA_3hr                        0\n",
       "RV_MA_12hr                       0\n",
       "hourly_return                    0\n",
       "vol_future                       0\n",
       "whale_net_usd                 5071\n",
       "whale_net_usd_24h             5071\n",
       "whale_burst_flag              5071\n",
       "etow_usd_log                  5071\n",
       "etow_coins_log                5071\n",
       "whale_txn_count_log           5071\n",
       "wtoe_usd_log                  5071\n",
       "wtoe_coins_log                5071\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btc_whale_alerts_df['datetime1h'] = pd.to_datetime(btc_whale_alerts_df['datetime1h'], utc=True)\n",
    "btc_dune_df['timestamp'] = pd.to_datetime(btc_dune_df['timestamp'], utc=True)\n",
    "\n",
    "btc_whale_alerts_df = btc_whale_alerts_df.rename(columns={'datetime1h': 'timestamp'})\n",
    "\n",
    "btc_whale_alerts_df = btc_whale_alerts_df.sort_values('timestamp')\n",
    "btc_dune_df = btc_dune_df.sort_values('timestamp')\n",
    "\n",
    "btc_merged_df = pd.merge_asof(\n",
    "    btc_dune_df,\n",
    "    btc_whale_alerts_df,\n",
    "    on='timestamp',\n",
    "    direction='nearest',          # get nearest match in time\n",
    "    tolerance=pd.Timedelta('1h')  # only merge if within 1 hour\n",
    ")\n",
    "\n",
    "btc_merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1499,
   "id": "154d82ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_merged_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1500,
   "id": "7248cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#btc_merged_df.to_csv('../Data/final_btc_df_var_fevd.csv', index=False)\n",
    "#eth_merged_df.to_csv('../Data/final_btc_df_var_fevd.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
